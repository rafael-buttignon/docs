# Criação de pipeline para fontes S3

Agora vamos ver os parametros que vamos precisar preencher para conseguirmos trazer dados de uma fonte S3. Lembrando que no momento só temos leitura de arquivos do tipo **Parquet**.

---
~~~yaml

  datasets:
    - name: 
      active: 
      domain: 
      entity: 
      task_owner: 
      owner_team:  
      file_path: 
      file_type: 
      has_header: 
      connection_id: 
      metadata:
      notifier:

~~~
---
- ``name``: Trata-se do nome do processo, podendo também ser o nome do arquivo que está sendo utilizado na ingestão, visando facilitar a identificação.

  - ``Validação``: obrigatório, não nulo e string. 
---
- ``domain``: Dominio de negocio em que a task esta representada.

  - ``Validação``: obrigatório, não nulo e string. 
---
- ``entity``: Entidade de negocio em que a task esta representada.

  - ``Validação``: obrigatório, não nulo e string. 
---
- ``task_owner``: E a pessoa ``responsável`` que vai cuidar da task.

  - ``Validação``: obrigatório, não nulo e string.
---
- ``owner_team``: E o time do ``responsável`` pela task. 

  - ``Validação``: obrigatório, não nulo, string e CamelCase.
---

- ``active``: E para saber se sua task vai estar ``ativa`` ou ``não`` no caso ``habilitar`` e ``desabilitar`` a task.

  - ``Validação``: obrigatório, ``True`` ou ``False``
---
- ``file_path`` E o caminho, aonde o dado está armazenado.
	- ``Validação``: obrigatório, não nulo, string.
---

- ``file_type``: E o tipo de arquivo, que vamos consumir.
	- ``Validação``: obrigatório, não nulo, string, aceita apenas 'Parquet'

---
- ``has_header``: Se o cabeçalho esta ativo. 
	- ``Validação``: obrigatório, não nulo, string.
  - ``Exemplo``: 'true' ou 'false'.
---
- ``connection_id``: O nome da ``connection`` criada na UI do ``airflow`` com as credenciais para a conexão com a fonte.

  - ``Validação``: obrigatório, não nulo, string e snake_case.

---
- ``notifier``: É onde você configura para notifica-lo sobre as ações na DAG.

  - ``Validação``: obrigatório, não nulo e dict. type aceito apenas: e-mail e ms_teams.
  - ``Exemplo``:   
 ~~~yaml
       notifier:
        type: "ms_teams"
        receivers:
          - 'IrisDataGovernanceWebhook'
 ~~~~ 
 ---
 **Esses são os parametros padões para sua conexão funcionar, agora temos alguns parametros a mais caso você precise**
 
 ---
- ``multiples_files``: Este parâmetro permite a ingestão de vários arquivos de uma pasta com o mesmo schema.
	- ``Validação``: opcional, não nulo. Aceito apenas ``True`` ou ``False``.
 ---

- ``column_separator``: E o delimitador que vamos usar para separar as colunas do dataset. 
	- ``Validação``: obrigatório, não nulo.
  - ``Exemplo``: ';' , '|'.
 ---
 - ``path_variables``: dicionário contendo todas as variáveis a serem customizadas pelo caminho dinâmico. Consulte como utilizar este parâmetro para tornar seu caminho dinamico [**clicando aqui**](https://iris.ambev.com.br/pt-br/data-ingestion/arq2-0/explanation/dynamic-paths).  
	- ``Validação``: opcional, não nulo, dict.

---
## Tipos de Ingestão de Arquivos


Para trabalhar com esses diversos tipos, é necessário configurar adequadamente o caminho (path) conforme desejado. Aqui estão alguns exemplos práticos:

1. Uma maneira simples é acessar o arquivo desejado usando o caminho especifico - ``Exemplo``:   
 ~~~yaml                         
 file_path: 'integration/consume_directory/FlyRNAi_data_baseline_vs_EGF.parquet'
 file_type: 'parquet'
 multiples_files: False
~~~
Nesse exemplo, estamos especificando o caminho do arquivo desejado: ***FlyRNAi_data_baseline_vs_EGF.parquet***. Essa abordagem permite a ingestão de um único arquivo específico. No entanto, é importante destacar que você pode tornar o caminho mais dinâmico ao utilizar parâmetros, permitindo maior flexibilidade na seleção dos arquivos a serem processados. [**Leia Sobre Caminho Dinâmico**](https://iris.ambev.com.br/pt-br/data-ingestion/arq2-0/explanation/dynamic-paths).

---

2. Uma segunda abordagem é ler o último arquivo adicionado em um caminho específico. Nesse caso, a regra irá apenas recuperar o arquivo mais recente na pasta. Se houver 100 arquivos dentro desse caminho, ele selecionará o arquivo mais recentemente adicionado. Isso significa que, se um novo arquivo for adicionado nesse caminho todos os dias, ele sempre pegará o arquivo mais recente, ou seja, o "último" arquivo. - ``Exemplo``:

 ~~~yaml                         
 file_path: 'integration/consume_directory/'
 file_type: 'parquet'
 multiples_files: False
~~~

Nessa abordagem, você irá recuperar o último arquivo adicionado no caminho **consume_directory/**. É importante deixar o caminho vazio após a pasta, pois o algoritmo irá selecionar automaticamente o arquivo mais recente encontrado nesse diretório. Ele buscará o último arquivo adicionado nesse diretório, independentemente do seu nome específico.

---


## Yaml de exemplo para fonte S3

Esta aqui um exemplo de como o seu <kbd>.yaml</kbd> vai ficar por padrão.

~~~yaml
dag:
  dag_type: 's3'
  dag_class: 'source'
  schedule: '0 * * * *'
  system: 'Awsteste'
  country: 'Brazil'
  datasets:
    - name: 'teste'
      active: True
      domain: 'Teste'
      entity: 'AwsTeste'
      task_owner: 'teste@ambevtech.com.br'
      owner_team: 'Teste'
      file_path: 'teste/test_path/*.parquet'
      file_type: 'parquet'
      has_header: True
      connection_id: 'S3_teste'
      metadata: !include ../metadata/metadata_test_awsteste.yaml
      notifier:
          - 'IrisDataGovernanceWebhook'
~~~

---
Agora você precisa criar sua Conexão do Airflow para a sua Dag funcionar, é simples, basta você seguir essa doc: [**Connection Airflow - S3**](https://iris.ambev.com.br/pt-br/iris-ingestion/getting-started/connection/S3)
